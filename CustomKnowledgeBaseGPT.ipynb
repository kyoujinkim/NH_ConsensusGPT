{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA/Lzi5WgZl4mkEWKtVHb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyoujinkim/NH_CustomKnowledgeBase_GPT/blob/master/CustomKnowledgeBaseGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd_i641REUHw"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain[llms]\n",
        "!pip install PyMuPDF\n",
        "!pip install sentence_transformers\n",
        "!pip install pip install chromadb\n",
        "!git clone https://github.com/kyoujinkim/NH_CustomKnowledgeBase_GPT.git\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/NH_CustomKnowledgeBase_GPT')\n",
        "\n",
        "import openai\n",
        "from langchain import PromptTemplate\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "#https://github.com/BM-K/Sentence-Embedding-is-all-you-need\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from readPDF import PDFReader\n",
        "from promptTemplate import loadTemplate\n",
        "from quoteChecker import change_quote_num, print_quote\n",
        "\n",
        "def setting(apiKey        :str                                  ,\n",
        "            pdfFolderPath :str                                  ,\n",
        "            embeddingAi   :str  ='BM-K/KoSimCSE-bert-multitask' ,\n",
        "            docSeparator  :str  ='. '                           ,\n",
        "            docSize       :int  =2                              ,\n",
        "            docOverlap    :int  =0                              ,\n",
        "            ):\n",
        "  '''\n",
        "  기본 세팅을 위한 전처리 프로세스\n",
        "  :param apiKey: OPEN AI에서 발급받은 api Key 입력\n",
        "  :param pdfFolderPath: colab 기준 백데이터(pdf) 폴더의 위치\n",
        "  :param embeddingAi: 임베딩에 사용할 hugging face 기반 ai의 directory\n",
        "  :param docSeparator: PDF 문서를 분할하기 위한 분할자\n",
        "  :param docSize: PDF 문서 분할 문장 단위\n",
        "  :param docOverlap: PDF 문서 분할시 오버랩 문장수\n",
        "  :return: langchain.vectorstores.chroma.Chroma 임베딩과 함께 저장된 Document 데이터베이스\n",
        "  '''\n",
        "\n",
        "  os.environ[\"OPENAI_API_KEY\"] = apiKey\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=embeddingAi)\n",
        "\n",
        "  if os.path.isdir(\"./ChromaDB/\"):  \n",
        "    docsearch = Chroma(embedding_function = embeddings, persist_directory=\"./ChromaDB/\")\n",
        "  else:\n",
        "    pr = PDFReader()\n",
        "    templates = loadTemplate()\n",
        "\n",
        "    pdftexts = pr.getPDF(datapath=pdfFolderPath)\n",
        "\n",
        "    docs_split = []\n",
        "    for doc in tqdm(pdftexts, desc='PDF 세부분할'):\n",
        "      doc_split= pr.split_text(doc,\n",
        "                              separator =docSeparator,\n",
        "                              size      =docSize,\n",
        "                              overlap   =docOverlap)\n",
        "      docs_split.extend(doc_split)\n",
        "\n",
        "\n",
        "    docsearch = Chroma.from_documents(docs_split, embeddings, persist_directory=\"./ChromaDB/\")\n",
        "\n",
        "  return docsearch\n",
        "\n",
        "def run_proc(query:str,\n",
        "             baseDocument,\n",
        "             llmAiEngine:str = 'gpt-3.5-turbo',\n",
        "             numberOfReason:int = 15,\n",
        "             iterNum:int = 5,\n",
        "             temperature:float = 0.5,\n",
        "             frequencyPenalty:float = 1.0,\n",
        "             ):\n",
        "  '''\n",
        "  쿼리에 대한 응답을 생성하는 텍스트 컨센서스 생성 프로세스\n",
        "  :param query: 쿼리 (스트링 형식)\n",
        "  :param baseDocumnet: setting process에서 생성한 크로마 데이터베이스\n",
        "  :param llmAiEngine: OpenAi에서 제공하는 LLM AI Engine\n",
        "  :param numberOfReason: 응답 생성을 위해 사용할 분할 문서의 개수\n",
        "  :param temperature: 텍스트 생성의 자유도, 창의도(이런 케이스에서는 낮은게 좋음)\n",
        "  :param frequencyPenalty: 반복 단어에 대한 벌점(높을수록 반복 회피)\n",
        "  :return: None\n",
        "  '''\n",
        "\n",
        "  openai.Engine = llmAiEngine\n",
        "\n",
        "  '''prompt 기본 서식 로딩'''\n",
        "  templates   = loadTemplate()\n",
        "  PROMPT      = PromptTemplate(template=templates['template'], input_variables=[\"summaries\", \"question\"])\n",
        "  PROMPT_S    = PromptTemplate(template=templates['template_s'], input_variables=[\"summaries\", \"question\"])\n",
        "  PROMPT_AGG  = PromptTemplate(template=templates['template_agg'], input_variables=[\"summaries\", \"question\"])\n",
        "\n",
        "  '''chain prompt 정의'''\n",
        "  chain     = load_qa_with_sources_chain(ChatOpenAI(model_name=       llmAiEngine, \n",
        "                                                    temperature=      0.0,\n",
        "                                                    frequency_penalty=0.0,\n",
        "                                                    ), \n",
        "                                         chain_type =\"stuff\",\n",
        "                                         prompt     =PROMPT)\n",
        "  chain_s   = load_qa_with_sources_chain(ChatOpenAI(model_name=       llmAiEngine,\n",
        "                                                    temperature=      0.5,\n",
        "                                                    frequency_penalty=1.0,\n",
        "                                                    ),\n",
        "                                         chain_type =\"stuff\",\n",
        "                                         prompt     =PROMPT_S)\n",
        "  chain_agg = load_qa_with_sources_chain(ChatOpenAI(model_name=       llmAiEngine,\n",
        "                                                    temperature=      1.0,\n",
        "                                                    frequency_penalty=0.0,\n",
        "                                                    ),\n",
        "                                         chain_type =\"stuff\",\n",
        "                                         prompt     =PROMPT_AGG)\n",
        "\n",
        "  '''쿼리와 유사한 문서 목록 획득'''\n",
        "  docs = baseDocument.similarity_search(query, k=numberOfReason)\n",
        "\n",
        "  '''근거 목록 저장'''\n",
        "  context_doc = []\n",
        "  for idx in range(0, int(numberOfReason/iterNum)):\n",
        "    summed_docs_part = docs[idx*iterNum:(idx+1)*iterNum]\n",
        "    output = chain({\"input_documents\": summed_docs_part, \"question\": query}, return_only_outputs=True)\n",
        "    output['output_text'] = change_quote_num(output['output_text'], idx*iterNum)\n",
        "    context_doc.append(Document(page_content=output['output_text'], metadata={\"source\": ''}))\n",
        "\n",
        "  '''필터링 근거 출력'''\n",
        "  output = chain_s({\"input_documents\": context_doc, \"question\": query}, return_only_outputs=True)\n",
        "  print(output['output_text'])\n",
        "  rearr_context_doc = Document(page_content=output['output_text'], metadata={\"source\": ''})\n",
        "\n",
        "  '''요약 기준 결론 출력'''\n",
        "  output = chain_agg({\"input_documents\": [rearr_context_doc], \"question\": query}, return_only_outputs=True)\n",
        "  print(output['output_text'])\n",
        "\n",
        "  '''주석 출력'''\n",
        "  print_quote(rearr_context_doc, docs)\n",
        "\n",
        "docsearch = setting(apiKey        ='YOUR API KEY',\n",
        "                    pdfFolderPath ='YOUR GOOGLE DRIVE PDF FOLDER PATH',\n",
        "                    embeddingAi   ='BM-K/KoSimCSE-bert-multitask',\n",
        "                    docSeparator  ='. ',\n",
        "                    docSize       =2,\n",
        "                    docOverlap    =0,\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_proc(query            ='국내 주식시장 전망',\n",
        "          baseDocument     =docsearch,\n",
        "          llmAiEngine      ='gpt-3.5-turbo',\n",
        "          numberOfReason   =15,\n",
        "          iterNum          =5,\n",
        "          temperature      =0.5,\n",
        "          frequencyPenalty =1.0\n",
        "         )"
      ],
      "metadata": {
        "id": "h0l1NbYODQ2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
